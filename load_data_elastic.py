from dotenv import load_dotenv
load_dotenv()

import time
import json
import uuid
import numpy as np
import tiktoken
from elasticsearch.helpers import bulk, parallel_bulk
from elasticsearch import Elasticsearch
import os
from qdrant_client import QdrantClient
from qdrant_client.http import models
from tqdm import tqdm
import logging
import redis
import torch
import gc
from embedding_model_manager import model_manager
from elasticsearch import helpers
from datetime import datetime
import threading
from progress_utils import safe_update_progress
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("qdrant_loader")

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏—è
redis_client = redis.Redis(host='localhost', port=6379, db=0)

es = Elasticsearch(
    hosts=["http://localhost:9200"],
    basic_auth=("elastic", "biz8z5i1w0nLPmEweKgP"),
    verify_certs=False,
    headers={"Accept": "application/vnd.elasticsearch+json; compatible-with=9"}
)

client_qdrant = QdrantClient("localhost", port=6333)

# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
MAX_TOKENS = 6000  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
OVERLAP = 150      # –£–º–µ–Ω—å—à–µ–Ω–æ
EMBED_BATCH_SIZE = 256  # –£–≤–µ–ª–∏—á–µ–Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ
QDRANT_BATCH_SIZE = 200  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è Qdrant
ES_BATCH_SIZE = 2000     # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è Elasticsearch

encoding = tiktoken.get_encoding("cl100k_base")

def split_text_into_chunks_optimized(text, max_tokens=MAX_TOKENS, overlap=OVERLAP):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    if not text or not text.strip():
        return []
        
    tokens = encoding.encode(text)
    if len(tokens) <= max_tokens:
        return [text]
    
    chunks = []
    step = max_tokens - overlap
    
    for i in range(0, len(tokens), step):
        chunk_tokens = tokens[i:i + max_tokens]
        if len(chunk_tokens) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            chunks.append(encoding.decode(chunk_tokens))
    
    return chunks

def process_documents_batch(documents_batch):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–µ–Ω–∞)"""
    results = []
    text_fields = ["text", "–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è", "title", "content", "message", "description"]
    
    for document in documents_batch:
        if not isinstance(document, dict):
            continue
            
        text = None
        for field in text_fields:
            if field in document and isinstance(document[field], str) and document[field].strip():
                text = document[field].strip()
                break
                
        if not text:
            continue
            
        chunks = split_text_into_chunks_optimized(text)
        if not chunks:
            continue
            
        metadata = document.copy()
        metadata["used_text_field"] = next(
            (field for field in text_fields if field in document and document[field] == text), None
        )
        
        doc_id = document.get('id', str(uuid.uuid4()))
        results.append((doc_id, text, chunks, metadata))
    
    return results

from concurrent.futures import ThreadPoolExecutor  # leave this

def batch_process_documents_with_embeddings_optimized(documents, task_id=None):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π"""
    if task_id:
        safe_update_progress(task_id, 30, stage="chunking", 
                           stage_details=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    try:
        logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–∞—Ç—á–∞–º–∏
        cpu_count = min(os.cpu_count() or 1, 4)
        batch_size = max(len(documents) // cpu_count, 100)
        document_batches = [documents[i:i + batch_size] for i in range(0, len(documents), batch_size)]

        if len(document_batches) > 1:
            with ThreadPoolExecutor(max_workers=cpu_count) as executor:
                batch_results = list(executor.map(process_documents_batch, document_batches))
            results = [item for br in batch_results for item in br]
        else:
            results = process_documents_batch(documents)
        
        if not results:
            logger.warning("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return []
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –ø–æ–ª—É—á–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        global_chunks = []
        index_info = []
        
        for doc_id, text, chunks, metadata in results:
            start = len(global_chunks)
            global_chunks.extend(chunks)
            end = len(global_chunks)
            index_info.append((doc_id, text, (start, end), metadata))
        
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(global_chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
        
        if task_id:
            safe_update_progress(task_id, 40, stage="embedding", 
                               stage_details=f"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è {len(global_chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –±–æ–ª—å—à–∏–º–∏ –±–∞—Ç—á–∞–º–∏
        all_vectors = []
        chunk_batch_size = EMBED_BATCH_SIZE
        total_batches = (len(global_chunks) + chunk_batch_size - 1) // chunk_batch_size
        
        for batch_idx in range(0, len(global_chunks), chunk_batch_size):
            batch_chunks = global_chunks[batch_idx:batch_idx + chunk_batch_size]
            
            try:
                batch_vectors = model_manager.encode_texts(
                    batch_chunks,
                    batch_size=chunk_batch_size,
                    normalize_embeddings=True
                )
                
                if isinstance(batch_vectors, np.ndarray):
                    batch_vectors = batch_vectors.tolist()
                elif not isinstance(batch_vectors, list):
                    batch_vectors = [batch_vectors] if batch_vectors is not None else []
                
                all_vectors.extend(batch_vectors)
                
                # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                if task_id:
                    progress = 40 + int(((batch_idx + chunk_batch_size) / len(global_chunks)) * 30)
                    safe_update_progress(task_id, progress, stage="embedding",
                                       stage_details=f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {min(batch_idx + chunk_batch_size, len(global_chunks))}/{len(global_chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
                
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω –±–∞—Ç—á {batch_idx//chunk_batch_size + 1}/{total_batches}")
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –±–∞—Ç—á–∞: {e}")
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–≥–æ –±–∞—Ç—á–∞
                all_vectors.extend([None] * len(batch_chunks))
        
        if task_id:
            safe_update_progress(task_id, 75, stage="preparing", 
                               stage_details="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
        
        # –ë—ã—Å—Ç—Ä–∞—è —Å–±–æ—Ä–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        processed_docs = []
        
        for doc_id, text, (start, end), metadata in index_info:
            chunk_vectors = [v for v in all_vectors[start:end] if v is not None and len(v) > 0]
            
            if not chunk_vectors:
                continue
            
            try:
                # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ
                avg_vector = np.mean(chunk_vectors, axis=0).tolist()
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è {doc_id}: {e}")
                continue
            
            doc_payload = {
                "content": text,
                "chunks": global_chunks[start:end] if (end - start) > 1 else None,
                "metadata": metadata
            }
            
            processed_doc = {
                "id": doc_id,
                "vector": avg_vector,
                "payload": doc_payload
            }
            processed_docs.append(processed_doc)
        
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(processed_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Qdrant")
        
        if task_id:
            safe_update_progress(task_id, 80, stage="preparing",
                               stage_details=f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(processed_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        return processed_docs
        
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ batch_process_documents_with_embeddings_optimized: {e}")
        return []
    finally:
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

def load_to_qdrant_optimized(collection_name, documents, task_id):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant"""
    if not documents:
        raise ValueError("–°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—É—Å—Ç!")
    
    try:
        logger.info(f"–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Qdrant")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        if not acquire_qdrant_lock(collection_name, task_id):
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –∫–æ–ª–ª–µ–∫—Ü–∏–∏")
        
        safe_update_progress(task_id, 80, stage="qdrant_preparation", 
                           stage_details="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –∑–∞–≥—Ä—É–∑–∫–µ –≤ Qdrant")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if not client_qdrant.collection_exists(collection_name):
            vector_size = len(documents[0]["vector"])
            logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name} —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é {vector_size}")
            
            client_qdrant.create_collection(
                collection_name=collection_name,
                vectors_config=models.VectorParams(
                    size=vector_size,
                    distance=models.Distance.COSINE
                ),
                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
                optimizers_config=models.OptimizersConfigDiff(
                    indexing_threshold=0,  # –û—Ç–∫–ª—é—á–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –≤–æ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏
                ),
                hnsw_config=models.HnswConfigDiff(
                    payload_m=16,
                    m=0  # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º HNSW
                )
            )
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –±–æ–ª—å—à–∏–º–∏ –±–∞—Ç—á–∞–º–∏
        batch_size = QDRANT_BATCH_SIZE
        total_docs = len(documents)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ —Ç–æ—á–∫–∏ —Å—Ä–∞–∑—É
        points = []
        for i, doc in enumerate(documents):
            if isinstance(doc["id"], str) and doc["id"].isdigit():
                point_id = int(doc["id"])
            else:
                point_id = hash(str(doc["id"])) % (2**31)  # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ
            
            points.append(
                models.PointStruct(
                    id=point_id,
                    vector=doc["vector"],
                    payload=doc["payload"]
                )
            )
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–∞–º–∏ —Å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–æ–º
        uploaded = 0
        for i in range(0, len(points), batch_size):
            batch = points[i:i + batch_size]
            
            try:
                client_qdrant.upsert(
                    collection_name=collection_name,
                    points=batch,
                    wait=False  # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                )
                
                uploaded += len(batch)
                progress = 85 + int((uploaded / total_docs) * 15)
                
                safe_update_progress(task_id, progress, stage="qdrant_upload",
                                   stage_details=f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {uploaded}/{total_docs} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                
                # logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –±–∞—Ç—á {i//batch_size + 1}, –≤—Å–µ–≥–æ: {uploaded}/{total_docs}")
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞—Ç—á–∞: {e}")
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –º–µ–Ω—å—à–∏–º –±–∞—Ç—á–µ–º
                if batch_size > 50:
                    batch_size = batch_size // 2
                    continue
                raise e
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
        time.sleep(1)
        
        # –í–∫–ª—é—á–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –æ–±—Ä–∞—Ç–Ω–æ
        client_qdrant.update_collection(
            collection_name=collection_name,
            optimizers_config=models.OptimizersConfigDiff(
                indexing_threshold=20000,
            )
        )
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {total_docs} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        safe_update_progress(task_id, 100, status="completed", stage="completed",
                           stage_details="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Qdrant: {e}")
        safe_update_progress(task_id, 0, status="failed", error=str(e))
        raise e
    finally:
        release_qdrant_lock(collection_name, task_id)

def load_file_to_elstic(filename, path=None, task_id=None):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞"""
    # logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞")
    
    if task_id is None:
        task_id = str(uuid.uuid4())
    
    try:
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π mapping –¥–ª—è Elasticsearch
        mapping = {
            "mappings": {
                "properties": {
                    "title": {"type": "text", "analyzer": "russian"},
                    "text": {"type": "text", "analyzer": "russian"},
                    "–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è": {"type": "text", "analyzer": "russian"},
                    "timeCreate": {"type": "long"},
                    "hub": {"type": "keyword"},
                    "city": {"type": "keyword"},
                    "audienceCount": {"type": "integer"},
                    "url": {"type": "text", "index": False}
                }
            },
            "settings": {
                "index": {
                    "mapping.total_fields.limit": 3000,
                    "mapping.ignore_malformed": True,
                    "number_of_shards": 1,
                    "number_of_replicas": 0,  # –û—Ç–∫–ª—é—á–∞–µ–º —Ä–µ–ø–ª–∏–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                    "refresh_interval": "30s",  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                    "translog": {
                        "flush_threshold_size": "1gb"
                    }
                }
            }
        }
        
        if path:
            os.chdir(path)
        
        file_name = filename.filename if hasattr(filename, 'filename') else filename
        new_index = file_name.replace('.json', '').lower()
        
        logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞: {new_index}")
        
        # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å
        if es.indices.exists(index=new_index):
            es.indices.delete(index=new_index, ignore=[400, 404])
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
        response = es.indices.create(index=new_index, body=mapping, ignore=400)
        
        if not ('acknowledged' in response and response['acknowledged']):
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {response}")
            return {"status": "failed", "error": "–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞"}
        
        # –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ JSON
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {file_name}")
        with open(file_name, 'r', encoding='utf-8') as file:
            data = json.load(file)
        
        if not isinstance(data, list) or not data:
            return {"status": "failed", "error": "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON"}
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ JSON")
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è bulk –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        actions = []
        for i, doc in enumerate(data):
            if not isinstance(doc, dict):
                continue
            
            doc_id = str(doc.get('id', doc.get('idExternal', str(uuid.uuid4()))))
            
            if not any(field in doc for field in ["text", "–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è", "title", "content"]):
                continue
            
            actions.append({
                "_index": new_index,
                "_id": doc_id,
                "_source": doc
            })
        
        if not actions:
            return {"status": "failed", "error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"}
        
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(actions)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è Elasticsearch")
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è bulk –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
        success_count = 0
        error_count = 0
        
        for success, info in parallel_bulk(
            es,
            actions,
            chunk_size=ES_BATCH_SIZE,
            max_chunk_bytes=50*1024*1024,  # 50MB –±–∞—Ç—á–∏
            thread_count=4,  # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø–æ—Ç–æ–∫–∏
            queue_size=8
        ):
            if not success:
                error_count += 1
                logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {info}")
            else:
                success_count += 1
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        es.indices.refresh(index=new_index)
        total_docs = es.count(index=new_index)['count']
        
        logger.info(f"‚úÖ Elasticsearch –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        logger.info(f"   –£—Å–ø–µ—à–Ω–æ: {success_count}, –û—à–∏–±–æ–∫: {error_count}, –í—Å–µ–≥–æ –≤ –∏–Ω–¥–µ–∫—Å–µ: {total_docs}")
        
        if total_docs == 0:
            return {"status": "failed", "error": "–ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏"}
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è Qdrant
        logger.info("üîÑ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è Qdrant")
        processed_docs = batch_process_documents_with_embeddings_optimized(data, task_id)
        
        if not processed_docs:
            return {"status": "failed", "error": "–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è Qdrant"}
        
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        logger.info(f"   –ò—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(data)}")
        logger.info(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–ª—è Qdrant: {len(processed_docs)}")
        logger.info(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {len(data) - len(processed_docs)}")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –≤ Redis
        redis_client.hset(
            f"task:{task_id}",
            mapping={
                "status": "processing",
                "progress": "80",
                "total": str(len(processed_docs)),
                "start_time": datetime.now().isoformat(),
                "total_docs": str(len(data))
            }
        )
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant
        try:
            load_to_qdrant_optimized(new_index, processed_docs, task_id)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Qdrant: {e}")
            return {"status": "failed", "error": str(e)}
        
        logger.info("üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
        return {
            "status": "completed",
            "task_id": task_id,
            "index_name": new_index,
            "processed_docs": len(processed_docs),
            "elasticsearch_docs": total_docs
        }
        
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return {"status": "failed", "error": str(e)}
    finally:
        try:
            model_manager.cleanup()
            # logger.info("‚úÖ –†–µ—Å—É—Ä—Å—ã –æ—á–∏—â–µ–Ω—ã")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
def acquire_qdrant_lock(collection_name, task_id, timeout=30):
    lock_key = f"qdrant_lock:{collection_name}"
    deadline = time.time() + timeout
    
    while time.time() < deadline:
        if redis_client.set(lock_key, task_id, nx=True, ex=120):  # 2 –º–∏–Ω—É—Ç—ã
            # logger.info(f"üîí –ü–æ–ª—É—á–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ {collection_name}")
            return True
        time.sleep(1)
    
    return False

def release_qdrant_lock(collection_name, task_id):
    lock_key = f"qdrant_lock:{collection_name}"
    owner = redis_client.get(lock_key)
    
    if owner and owner.decode('utf-8') == task_id:
        redis_client.delete(lock_key)
        # logger.info(f"üîì –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ {collection_name}")
        return True
    return False