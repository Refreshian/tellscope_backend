from dotenv import load_dotenv
load_dotenv()

import time
import json
import uuid
import numpy as np
import tiktoken
from elasticsearch.helpers import bulk, parallel_bulk
from elasticsearch import Elasticsearch
import os
from qdrant_client import QdrantClient
from qdrant_client.http import models
from tqdm import tqdm
import logging
import redis
import torch
import gc
from embedding_model_manager import model_manager
from elasticsearch import helpers
from datetime import datetime
import threading
from progress_utils import safe_update_progress
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("qdrant_loader")

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏—è
redis_client = redis.Redis(host='localhost', port=6379, db=0)

es = Elasticsearch(
    hosts=["http://localhost:9200"],
    basic_auth=("elastic", "biz8z5i1w0nLPmEweKgP"),
    verify_certs=False,
    headers={"Accept": "application/vnd.elasticsearch+json; compatible-with=9"},
    # –û—Ç–∫–ª—é—á–∞–µ–º sniffing - —ç—Ç–æ –≤–∞–∂–Ω–æ!
    sniff_on_start=False,
    sniff_on_node_failure=False,
    sniff_before_requests=False,
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–∞–π–º–∞—É—Ç–æ–≤
    request_timeout=30,
    max_retries=3,
    retry_on_timeout=True
)

# –î–æ–±–∞–≤—å—Ç–µ –≤ –Ω–∞—á–∞–ª–æ load_data_elastic.py –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
try:
    info = es.info()
    logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Elasticsearch: {info['version']['number']}")
except Exception as e:
    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Elasticsearch: {e}")

client_qdrant = QdrantClient("localhost", port=6333)

# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
MAX_TOKENS = 6000
OVERLAP = 150
EMBED_BATCH_SIZE = 256
QDRANT_BATCH_SIZE = 200
ES_BATCH_SIZE = 2000

encoding = tiktoken.get_encoding("cl100k_base")

def safe_bulk_index(actions, index_name, max_retries=3):
    for attempt in range(max_retries):
        try:
            success, errors = helpers.bulk(
                es,
                actions,
                chunk_size=100,
                request_timeout=120,
                raise_on_error=False
            )
            return success, errors
        except ConnectionError as e:
            if attempt == max_retries - 1:
                raise e
            logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å, –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∞–µ–º—Å—è...")
            time.sleep(2 ** attempt)
            
def split_text_into_chunks_optimized(text, max_tokens=MAX_TOKENS, overlap=OVERLAP):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    if not text or not isinstance(text, str) or not text.strip():
        return []
    
    try:
        tokens = encoding.encode(text)
        
        if not tokens or len(tokens) == 0:
            return []
            
        if len(tokens) <= max_tokens:
            return [text]
        
        chunks = []
        step = max_tokens - overlap
        
        for i in range(0, len(tokens), step):
            chunk_tokens = tokens[i:i + max_tokens]
            
            if chunk_tokens and isinstance(chunk_tokens, list) and len(chunk_tokens) > 50:
                try:
                    decoded_chunk = encoding.decode(chunk_tokens)
                    if decoded_chunk and decoded_chunk.strip():
                        chunks.append(decoded_chunk)
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —á–∞–Ω–∫–∞: {e}")
                    continue
        
        return chunks
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞–∑–±–∏–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
        return []

def validate_document_numeric_fields(doc):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ–ª–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    if not isinstance(doc, dict):
        return doc
    
    numeric_fields = ["timeCreate", "audienceCount"]
    
    for field in numeric_fields:
        if field in doc:
            original_value = doc[field]
            
            # üîç –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï
            logger.debug(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª—è {field}: value={original_value}, type={type(original_value)}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è
            if original_value is None or original_value == "" or original_value == "null":
                logger.warning(f"–ü–æ–ª–µ {field} —Å–æ–¥–µ—Ä–∂–∏—Ç None/–ø—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 0")
                doc[field] = 0
            else:
                try:
                    if isinstance(original_value, (int, float)):
                        if np.isnan(original_value) or np.isinf(original_value):
                            logger.warning(f"–ü–æ–ª–µ {field} —Å–æ–¥–µ—Ä–∂–∏—Ç NaN/Inf, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 0")
                            doc[field] = 0
                        else:
                            doc[field] = float(original_value)
                    elif isinstance(original_value, str):
                        cleaned = original_value.strip()
                        if cleaned and cleaned.lower() not in ['none', 'null', 'nan']:
                            cleaned = cleaned.replace(',', '.').replace(' ', '')
                            if cleaned.replace('.', '').replace('-', '').replace('+', '').isdigit():
                                converted = float(cleaned)
                                if np.isnan(converted) or np.isinf(converted):
                                    logger.warning(f"–ü–æ–ª–µ {field} –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ NaN/Inf, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 0")
                                    doc[field] = 0
                                else:
                                    doc[field] = converted
                            else:
                                logger.warning(f"–ü–æ–ª–µ {field} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–º: '{cleaned}', —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 0")
                                doc[field] = 0
                        else:
                            logger.warning(f"–ü–æ–ª–µ {field} –ø—É—Å—Ç–æ–µ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 0")
                            doc[field] = 0
                    else:
                        logger.warning(f"–ü–æ–ª–µ {field} –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞: {type(original_value)}, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 0")
                        doc[field] = 0
                        
                except (ValueError, TypeError, AttributeError) as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ {field}: {original_value} -> {e}, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 0")
                    doc[field] = 0
            
            # üîç –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê
            final_value = doc[field]
            logger.debug(f"–ü–æ–ª–µ {field} –ø–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {final_value}, type={type(final_value)}")
            
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            if final_value is None:
                logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ü–æ–ª–µ {field} –≤—Å–µ –µ—â–µ None –ø–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏!")
                doc[field] = 0
    
    return doc

def process_documents_batch(documents_batch):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    results = []
    text_fields = ["text", "–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è", "title", "content", "message", "description"]
    
    for idx, document in enumerate(documents_batch):
        try:
            if not isinstance(document, dict):
                logger.warning(f"–î–æ–∫—É–º–µ–Ω—Ç {idx} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º: {type(document)}")
                continue
            
            # üîç –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –î–û –í–ê–õ–ò–î–ê–¶–ò–ò
            logger.debug(f"–î–æ–∫—É–º–µ–Ω—Ç {idx} –î–û –≤–∞–ª–∏–¥–∞—Ü–∏–∏: timeCreate={document.get('timeCreate')}, audienceCount={document.get('audienceCount')}")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            document = validate_document_numeric_fields(document)
            
            # üîç –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ü–û–°–õ–ï –í–ê–õ–ò–î–ê–¶–ò–ò
            logger.debug(f"–î–æ–∫—É–º–µ–Ω—Ç {idx} –ü–û–°–õ–ï –≤–∞–ª–∏–¥–∞—Ü–∏–∏: timeCreate={document.get('timeCreate')}, audienceCount={document.get('audienceCount')}")
            
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            for field in ["timeCreate", "audienceCount"]:
                if field in document:
                    value = document[field]
                    if value is None:
                        logger.error(f"‚ùå –ù–ê–ô–î–ï–ù None –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ {idx} –ø–æ–ª–µ {field} –ü–û–°–õ–ï –≤–∞–ª–∏–¥–∞—Ü–∏–∏!")
                        document[field] = 0
                    # üîç –ü–†–û–í–ï–†–ö–ê –ù–ê –í–û–ó–ú–û–ñ–ù–û–°–¢–¨ –°–†–ê–í–ù–ï–ù–ò–Ø
                    try:
                        _ = value < 0  # –ü—Ä–æ–±—É–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
                    except TypeError as te:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ {idx} –ø–æ–ª–µ {field}: {te}")
                        logger.error(f"   –ó–Ω–∞—á–µ–Ω–∏–µ: {value}, —Ç–∏–ø: {type(value)}")
                        document[field] = 0
            
            # –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è
            text = None
            for field in text_fields:
                if field in document:
                    field_value = document[field]
                    if isinstance(field_value, str) and field_value.strip():
                        text = field_value.strip()
                        break
            
            if not text:
                continue
            
            # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏
            chunks = split_text_into_chunks_optimized(text)
            
            if not chunks or len(chunks) == 0:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document.get('id', 'unknown')}")
                continue
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            metadata = document.copy()
            
            # üîç –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ú–ï–¢–ê–î–ê–ù–ù–´–•
            for key in ["timeCreate", "audienceCount"]:
                if key in metadata and metadata[key] is None:
                    logger.error(f"‚ùå –ù–∞–π–¥–µ–Ω None –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª—é—á–∞ {key}")
                    metadata[key] = 0
            
            metadata["used_text_field"] = next(
                (field for field in text_fields if field in document and document.get(field) == text), 
                None
            )
            
            doc_id = document.get('id') or document.get('idExternal') or str(uuid.uuid4())
            results.append((doc_id, text, chunks, metadata))
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {idx}: {e}", exc_info=True)
            continue
    
    return results

def batch_process_documents_with_embeddings_optimized(documents, task_id=None):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    if task_id:
        safe_update_progress(task_id, 30, stage="chunking", 
                           stage_details=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    try:
        logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # üîç –ü–†–û–í–ï–†–ö–ê –í–•–û–î–ù–´–• –î–ê–ù–ù–´–•
        logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤—ã—Ö 3 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ None –∑–Ω–∞—á–µ–Ω–∏—è...")
        for i, doc in enumerate(documents[:3]):
            if isinstance(doc, dict):
                for field in ["timeCreate", "audienceCount"]:
                    if field in doc:
                        value = doc[field]
                        logger.info(f"  –î–æ–∫—É–º–µ–Ω—Ç {i}, –ø–æ–ª–µ {field}: {value} (type: {type(value)})")
                        if value is None:
                            logger.error(f"  ‚ùå –ù–ê–ô–î–ï–ù None –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        cpu_count = min(os.cpu_count() or 1, 4)
        batch_size = max(len(documents) // cpu_count, 100)
        document_batches = [documents[i:i + batch_size] for i in range(0, len(documents), batch_size)]

        if len(document_batches) > 1:
            with ThreadPoolExecutor(max_workers=cpu_count) as executor:
                batch_results = list(executor.map(process_documents_batch, document_batches))
            results = [item for br in batch_results for item in br]
        else:
            results = process_documents_batch(documents)
        
        if not results:
            logger.warning("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return []
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        global_chunks = []
        index_info = []
        
        for doc_id, text, chunks, metadata in results:
            # üîç –ü–†–û–í–ï–†–ö–ê –ú–ï–¢–ê–î–ê–ù–ù–´–•
            for field in ["timeCreate", "audienceCount"]:
                if field in metadata:
                    value = metadata[field]
                    if value is None:
                        logger.error(f"‚ùå None –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id} –ø–æ–ª–µ {field}")
                        metadata[field] = 0
            
            start = len(global_chunks)
            global_chunks.extend(chunks)
            end = len(global_chunks)
            index_info.append((doc_id, text, (start, end), metadata))
        
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(global_chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
        
        if task_id:
            safe_update_progress(task_id, 40, stage="embedding", 
                               stage_details=f"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è {len(global_chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        all_vectors = []
        chunk_batch_size = EMBED_BATCH_SIZE
        total_batches = (len(global_chunks) + chunk_batch_size - 1) // chunk_batch_size
        
        for batch_idx in range(0, len(global_chunks), chunk_batch_size):
            batch_chunks = global_chunks[batch_idx:batch_idx + chunk_batch_size]
            
            try:
                batch_vectors = model_manager.encode_texts(
                    batch_chunks,
                    batch_size=chunk_batch_size,
                    normalize_embeddings=True
                )
                
                if isinstance(batch_vectors, np.ndarray):
                    batch_vectors = batch_vectors.tolist()
                elif not isinstance(batch_vectors, list):
                    batch_vectors = [batch_vectors] if batch_vectors is not None else []
                
                all_vectors.extend(batch_vectors)
                
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                if task_id:
                    progress = 40 + int(((batch_idx + chunk_batch_size) / len(global_chunks)) * 30) if global_chunks else 40
                    safe_update_progress(task_id, progress, stage="embedding",
                                       stage_details=f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {min(batch_idx + chunk_batch_size, len(global_chunks))}/{len(global_chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
                
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω –±–∞—Ç—á {batch_idx//chunk_batch_size + 1}/{total_batches}")
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –±–∞—Ç—á–∞: {e}")
                all_vectors.extend([None] * len(batch_chunks))
        
        if task_id:
            safe_update_progress(task_id, 75, stage="preparing", 
                               stage_details="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
        
        # –°–±–æ—Ä–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        processed_docs = []
        
        for doc_id, text, (start, end), metadata in index_info:
            # üîç –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ü–ï–†–ï–î –î–û–ë–ê–í–õ–ï–ù–ò–ï–ú
            for field in ["timeCreate", "audienceCount"]:
                if field in metadata:
                    value = metadata[field]
                    if value is None:
                        logger.error(f"‚ùå None –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö {doc_id} –ø–æ–ª–µ {field}")
                        metadata[field] = 0
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                    try:
                        _ = value < 0
                    except TypeError as te:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {te}")
                        logger.error(f"   doc_id={doc_id}, field={field}, value={value}, type={type(value)}")
                        metadata[field] = 0
            
            chunk_vectors = [v for v in all_vectors[start:end] if v is not None and len(v) > 0]
            
            if not chunk_vectors:
                continue
            
            try:
                avg_vector = np.mean(chunk_vectors, axis=0).tolist()
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è {doc_id}: {e}")
                continue
            
            doc_payload = {
                "content": text,
                "chunks": global_chunks[start:end] if (end - start) > 1 else None,
                "metadata": metadata
            }
            
            processed_doc = {
                "id": doc_id,
                "vector": avg_vector,
                "payload": doc_payload
            }
            processed_docs.append(processed_doc)
        
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(processed_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Qdrant")
        
        if task_id:
            safe_update_progress(task_id, 80, stage="preparing",
                               stage_details=f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(processed_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        return processed_docs
        
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ batch_process_documents_with_embeddings_optimized: {e}", exc_info=True)
        return []
    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

def load_to_qdrant_optimized(collection_name, documents, task_id):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant"""
    if not documents:
        raise ValueError("–°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—É—Å—Ç!")
    
    try:
        logger.info(f"–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Qdrant")
        
        if not acquire_qdrant_lock(collection_name, task_id):
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –∫–æ–ª–ª–µ–∫—Ü–∏–∏")
        
        safe_update_progress(task_id, 80, stage="qdrant_preparation", 
                           stage_details="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –∑–∞–≥—Ä—É–∑–∫–µ –≤ Qdrant")
        
        if not client_qdrant.collection_exists(collection_name):
            vector_size = len(documents[0]["vector"])
            logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name} —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é {vector_size}")
            
            client_qdrant.create_collection(
                collection_name=collection_name,
                vectors_config=models.VectorParams(
                    size=vector_size,
                    distance=models.Distance.COSINE
                ),
                optimizers_config=models.OptimizersConfigDiff(
                    indexing_threshold=0,
                ),
                hnsw_config=models.HnswConfigDiff(
                    payload_m=16,
                    m=0
                )
            )
        
        batch_size = QDRANT_BATCH_SIZE
        total_docs = len(documents)
        
        points = []
        for i, doc in enumerate(documents):
            if isinstance(doc["id"], str) and doc["id"].isdigit():
                point_id = int(doc["id"])
            else:
                point_id = hash(str(doc["id"])) % (2**31)
            
            points.append(
                models.PointStruct(
                    id=point_id,
                    vector=doc["vector"],
                    payload=doc["payload"]
                )
            )
        
        uploaded = 0
        for i in range(0, len(points), batch_size):
            batch = points[i:i + batch_size]
            
            try:
                client_qdrant.upsert(
                    collection_name=collection_name,
                    points=batch,
                    wait=False
                )
                
                uploaded += len(batch)
                progress = 85 + int((uploaded / total_docs) * 15) if total_docs > 0 else 85
                
                safe_update_progress(task_id, progress, stage="qdrant_upload",
                                   stage_details=f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {uploaded}/{total_docs} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞—Ç—á–∞: {e}")
                if batch_size > 50:
                    batch_size = batch_size // 2
                    continue
                raise e
        
        time.sleep(1)
        
        client_qdrant.update_collection(
            collection_name=collection_name,
            optimizers_config=models.OptimizersConfigDiff(
                indexing_threshold=20000,
            )
        )
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {total_docs} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        safe_update_progress(task_id, 100, status="completed", stage="completed",
                           stage_details="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Qdrant: {e}", exc_info=True)
        safe_update_progress(task_id, 0, status="failed", error=str(e))
        raise e
    finally:
        release_qdrant_lock(collection_name, task_id)

def load_file_to_elstic(filename, path=None, task_id=None):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    if task_id is None:
        task_id = str(uuid.uuid4())
    
    try:
        mapping = {
            "mappings": {
                "properties": {
                    "title": {"type": "text", "analyzer": "russian"},
                    "text": {"type": "text", "analyzer": "russian"},
                    "–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è": {"type": "text", "analyzer": "russian"},
                    "timeCreate": {"type": "long"},
                    "hub": {"type": "keyword"},
                    "city": {"type": "keyword"},
                    "audienceCount": {"type": "integer"},
                    "url": {"type": "text", "index": False}
                }
            },
            "settings": {
                "index": {
                    "mapping.total_fields.limit": 3000,
                    "mapping.ignore_malformed": True,
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "refresh_interval": "30s",
                    "translog": {
                        "flush_threshold_size": "1gb"
                    }
                }
            }
        }
        
        if path:
            os.chdir(path)
        
        file_name = filename.filename if hasattr(filename, 'filename') else filename
        new_index = file_name.replace('.json', '').lower()
        
        logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞: {new_index}")
        
        if es.indices.exists(index=new_index):
            es.indices.delete(index=new_index, ignore=[400, 404])
        
        response = es.indices.create(index=new_index, body=mapping, ignore=400)
        
        if not ('acknowledged' in response and response['acknowledged']):
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {response}")
            return {"status": "failed", "error": "–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞"}
        
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {file_name}")
        with open(file_name, 'r', encoding='utf-8') as file:
            data = json.load(file)

        if not isinstance(data, list) or not data:
            return {"status": "failed", "error": "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON"}

        # üîç –î–ï–¢–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ò–°–•–û–î–ù–´–• –î–ê–ù–ù–´–•
        logger.info("=" * 50)
        logger.info("–ü–†–û–í–ï–†–ö–ê –ò–°–•–û–î–ù–´–• –î–ê–ù–ù–´–• –ò–ó JSON")
        logger.info("=" * 50)
        
        for i, doc in enumerate(data[:5]):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 5 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            if isinstance(doc, dict):
                logger.info(f"\n–î–æ–∫—É–º–µ–Ω—Ç {i}:")
                for field in ["timeCreate", "audienceCount"]:
                    if field in doc:
                        value = doc[field]
                        logger.info(f"  {field}: {value} (type: {type(value).__name__})")
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ None
                        if value is None:
                            logger.error(f"  ‚ùå –ù–ê–ô–î–ï–ù None –í –ò–°–•–û–î–ù–û–ú JSON!")
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                        try:
                            _ = value < 0
                            logger.info(f"  ‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ")
                        except TypeError as te:
                            logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {te}")

        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        cleaned_data = []
        for idx, doc in enumerate(data):
            if isinstance(doc, dict):
                # –õ–æ–≥–∏—Ä—É–µ–º –î–û –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                if idx < 3:
                    logger.info(f"\n–û—á–∏—Å—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {idx} –î–û –≤–∞–ª–∏–¥–∞—Ü–∏–∏:")
                    logger.info(f"  timeCreate: {doc.get('timeCreate')} (type: {type(doc.get('timeCreate')).__name__})")
                    logger.info(f"  audienceCount: {doc.get('audienceCount')} (type: {type(doc.get('audienceCount')).__name__})")
                
                doc = validate_document_numeric_fields(doc)
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ü–û–°–õ–ï –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                if idx < 3:
                    logger.info(f"  –ü–û–°–õ–ï –≤–∞–ª–∏–¥–∞—Ü–∏–∏:")
                    logger.info(f"  timeCreate: {doc.get('timeCreate')} (type: {type(doc.get('timeCreate')).__name__})")
                    logger.info(f"  audienceCount: {doc.get('audienceCount')} (type: {type(doc.get('audienceCount')).__name__})")
                
                cleaned_data.append(doc)
            else:
                logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç {idx} –Ω–µ–≤–µ—Ä–Ω–æ–≥–æ —Ç–∏–ø–∞: {type(doc)}")

        data = cleaned_data
        logger.info(f"–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(data)} –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Elasticsearch
        try:
            from elasticsearch.helpers import streaming_bulk
            
            def actions_generator():
                for doc in data:
                    if not isinstance(doc, dict):
                        continue
                    
                    doc_id = str(doc.get('id', doc.get('idExternal', str(uuid.uuid4()))))
                    
                    if not any(field in doc for field in ["text", "–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è", "title", "content"]):
                        continue
                    
                    yield {
                        "_index": new_index,
                        "_id": doc_id,
                        "_source": doc
                    }
            
            success_count = 0
            for ok, response in streaming_bulk(
                es,
                actions_generator(),
                chunk_size=200,
                max_retries=3,
                initial_backoff=2,
                yield_ok=False,
                raise_on_error=False
            ):
                if ok:
                    success_count += 1
                else:
                    logger.warning(f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {response}")
                    
        except Exception as bulk_error:
            logger.error(f"–û—à–∏–±–∫–∞ bulk –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {bulk_error}", exc_info=True)
        
        es.indices.refresh(index=new_index)
        total_docs = es.count(index=new_index)['count']
        
        logger.info(f"‚úÖ Elasticsearch –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        logger.info(f"   –£—Å–ø–µ—à–Ω–æ: {success_count}, –í—Å–µ–≥–æ –≤ –∏–Ω–¥–µ–∫—Å–µ: {total_docs}")
        
        if total_docs == 0:
            return {"status": "failed", "error": "–ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏"}
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è Qdrant
        logger.info("üîÑ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è Qdrant")
        processed_docs = batch_process_documents_with_embeddings_optimized(data, task_id)
        
        if not processed_docs:
            return {"status": "failed", "error": "–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è Qdrant"}
        
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        logger.info(f"   –ò—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(data)}")
        logger.info(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–ª—è Qdrant: {len(processed_docs)}")
        logger.info(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {len(data) - len(processed_docs)}")
        
        redis_client.hset(
            f"task:{task_id}",
            mapping={
                "status": "processing",
                "progress": "80",
                "total": str(len(processed_docs)),
                "start_time": datetime.now().isoformat(),
                "total_docs": str(len(data))
            }
        )
        
        try:
            load_to_qdrant_optimized(new_index, processed_docs, task_id)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Qdrant: {e}", exc_info=True)
            return {"status": "failed", "error": str(e)}
        
        logger.info("üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
        return {
            "status": "completed",
            "task_id": task_id,
            "index_name": new_index,
            "processed_docs": len(processed_docs),
            "elasticsearch_docs": total_docs
        }
        
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        return {"status": "failed", "error": str(e)}
    finally:
        try:
            model_manager.cleanup()
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")

def acquire_qdrant_lock(collection_name, task_id, timeout=30):
    lock_key = f"qdrant_lock:{collection_name}"
    deadline = time.time() + timeout
    
    while time.time() < deadline:
        if redis_client.set(lock_key, task_id, nx=True, ex=120):
            return True
        time.sleep(1)
    
    return False

def release_qdrant_lock(collection_name, task_id):
    lock_key = f"qdrant_lock:{collection_name}"
    owner = redis_client.get(lock_key)
    
    if owner and owner.decode('utf-8') == task_id:
        redis_client.delete(lock_key)
        return True
    return False