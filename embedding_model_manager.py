import torch
import gc
import logging
from sentence_transformers import SentenceTransformer
import numpy as np
import os
import time
import threading
import subprocess

logger = logging.getLogger(__name__)

async def force_clear_gpu():
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –Ω–∞ GPU"""
    try:
        # 1. –û—á–∏—Å—Ç–∫–∞ —á–µ—Ä–µ–∑ PyTorch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
        
        # 2. –£–±–∏–≤–∞–µ–º –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã Python, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ GPU
        result = subprocess.run(
            "nvidia-smi --query-compute-apps=pid --format=csv,noheader | xargs -r kill -9",
            shell=True,
            capture_output=True,
            text=True
        )
        logging.info(f"–û—á–∏—Å—Ç–∫–∞ GPU: {result.stdout or '–£—Å–ø–µ—à–Ω–æ'}")
        
        # 3. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —á–µ—Ä–µ–∑ NVIDIA-SMI
        os.system("nvidia-smi --gpu-reset -i 3")
        return {"status": "GPU memory cleared"}
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ GPU: {e}")
        raise

class ModelManager:
    _instance = None
    _model = None
    _initialized = False
    _lock = threading.Lock()
    _model_lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ModelManager, cls).__new__(cls)
        return cls._instance
    
    def __init__(self, device_id=None):  # üî• –ò–∑–º–µ–Ω–µ–Ω–æ: device_id –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None
        with self._lock:
            if not self._initialized:
                os.chdir('/home/dev/tellscope_app/tellscope_backend/data/embed_models')
                self.model_path = "deepvk/USER2-base"
                self.device = None  # üî• –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è
                self.preferred_device_id = device_id  # üî• –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π ID
                self._initialized = True
    
    def encode_texts(self, texts, batch_size=32, **kwargs):
        with self._model_lock:
            logger.info(f"üîí –ü–æ–ª—É—á–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
            
            model = self.get_model()
            self.clear_cuda_memory()
            
            if isinstance(texts, str):
                texts = [texts]
            if len(texts) == 0:
                return np.array([])

            orig_batch_size = batch_size
            while batch_size >= 1:
                try:
                    logger.info(f"–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ —Å batch_size={batch_size}")
                    embeddings = model.encode(
                        texts,
                        convert_to_tensor=False,
                        normalize_embeddings=kwargs.get('normalize_embeddings', True),
                        show_progress_bar=False,
                        batch_size=batch_size
                    )
                    logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
                    return embeddings
                except RuntimeError as e:
                    if "CUDA out of memory" in str(e):
                        logger.warning(f"CUDA out of memory –ø—Ä–∏ batch_size={batch_size}. –£–º–µ–Ω—å—à–∞–µ–º batch...")
                        self.clear_cuda_memory()
                        batch_size = batch_size // 2
                        if batch_size < 1:
                            logger.error("–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–¥–æ–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–π batch_size (<1).")
                            raise
                        time.sleep(0.5)
                    else:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
                        if self.device != 'cpu':
                            logger.info("–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ CPU –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏")
                            self.device = 'cpu'
                            self._model = None
                            return self.encode_texts(texts, batch_size=orig_batch_size, **kwargs)
                        raise e
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
                    raise e
    
    def initialize_model(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GPU —Å —É—á–µ—Ç–æ–º –≤–∏–¥–∏–º–æ—Å—Ç–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤"""
        try:
            import multiprocessing
            current_process = multiprocessing.current_process()
            
            if not torch.cuda.is_available():
                logger.info("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
                return 'cpu'
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
            os.environ['TORCH_USE_CUDA_DSA'] = '1'
            
            available_gpus = torch.cuda.device_count()
            if available_gpus == 0:
                logger.info("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
                return 'cpu'
            
            logger.info(f"–î–æ—Å—Ç—É–ø–Ω–æ GPU: {available_gpus}")
            
            # üî• –í–ê–ñ–ù–û: –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if self.preferred_device_id is not None:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã–π device_id –¥–æ—Å—Ç—É–ø–µ–Ω –≤ PyTorch
                if self.preferred_device_id >= available_gpus:
                    logger.warning(f"GPU {self.preferred_device_id} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–¥–æ—Å—Ç—É–ø–Ω–æ {available_gpus}). –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π.")
                    device_id = available_gpus - 1
                else:
                    device_id = self.preferred_device_id
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π GPU (–æ–±—ã—á–Ω–æ —ç—Ç–æ –Ω—É–∂–Ω—ã–π –Ω–∞–º GPU 3)
                device_id = available_gpus - 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∞–ª—å–Ω—É—é –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
            try:
                torch.cuda.set_device(device_id)
                device = f'cuda:{device_id}'
                
                # –¢–µ—Å—Ç–æ–≤–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è
                test_tensor = torch.randn(10, 10, device=device)
                _ = torch.mm(test_tensor, test_tensor.T)
                del test_tensor
                torch.cuda.empty_cache()
                
                logger.info(f"‚úÖ GPU {device} —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ {current_process.name}")
                return device
                
            except RuntimeError as e:
                if "invalid device ordinal" in str(e):
                    logger.warning(f"GPU {device_id} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                    # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π GPU
                    if device_id != 0:
                        device_id = 0
                        torch.cuda.set_device(device_id)
                        device = f'cuda:{device_id}'
                        
                        test_tensor = torch.randn(10, 10, device=device)
                        _ = torch.mm(test_tensor, test_tensor.T)
                        del test_tensor
                        torch.cuda.empty_cache()
                        
                        logger.info(f"‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–∏–ª–∏—Å—å –Ω–∞ GPU {device}")
                        return device
                    else:
                        raise e
                else:
                    raise e
            
        except Exception as e:
            logger.warning(f"GPU –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
            return 'cpu'
    
    def clear_cuda_memory(self):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ CUDA –ø–∞–º—è—Ç–∏"""
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è –≤—Å–µ—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤
                for i in range(torch.cuda.device_count()):
                    with torch.cuda.device(i):
                        torch.cuda.empty_cache()
                        torch.cuda.ipc_collect()
            
            gc.collect()
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ CUDA –ø–∞–º—è—Ç–∏: {e}")
    
    def get_model(self):
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ –≤–æ—Ä–∫–µ—Ä –ø—Ä–æ—Ü–µ—Å—Å–µ"""
        with self._lock:
            if self._model is None:
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –æ–±—Ä–∞—â–µ–Ω–∏–∏
                if self.device is None:
                    self.device = self.initialize_model()
                
                logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
                
                try:
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
                    os.chdir('/home/dev/tellscope_app/tellscope_backend/data/embed_models')
                    self._model = SentenceTransformer(
                        self.model_path,
                        device=self.device,
                        cache_folder='/tmp/sentence_transformers',
                        # local_files_only=True
                    )
                    
                    # –¢–µ—Å—Ç–æ–≤–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
                    test_embedding = self._model.encode(
                        ["—Ç–µ—Å—Ç"], 
                        show_progress_bar=False,
                        batch_size=1
                    )
                    del test_embedding
                    
                    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {self.device}")
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                    if self.device != 'cpu':
                        logger.info("–ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU")
                        self.device = 'cpu'
                        self._model = None
                        return self.get_model()
                    raise e
                    
            return self._model

    def cleanup(self):
        """–£–¥–∞–ª—è–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏, –æ—á–∏—â–∞–µ—Ç CUDA"""
        with self._lock:
            if self._model is not None:
                try:
                    del self._model
                except Exception:
                    pass
                self._model = None
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.ipc_collect()
                gc.collect()
        self.device = None

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä - –±–µ–∑ –ø–µ—Ä–µ–¥–∞—á–∏ device_id
model_manager = ModelManager()